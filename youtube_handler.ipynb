{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School\\HUST\\IT3180 Introdution of Software\\Project\\software\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: ffmpeg not found. The downloaded format may not be the best available. Installing ffmpeg is strongly recommended: https://github.com/yt-dlp/yt-dlp#dependencies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created at: ./json_output\\Cách_Tự_Học_Tiếng_Anh__Áp_dụng_cho_mọi_người_phương_pháp__tài_liệu_gợi_ý_VyVocab_Ep110.json\n",
      "Generated JSON file: ./json_output\\Cách_Tự_Học_Tiếng_Anh__Áp_dụng_cho_mọi_người_phương_pháp__tài_liệu_gợi_ý_VyVocab_Ep110.json\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained embedding model once\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def get_youtube_transcript(url: str) -> tuple[List[list[Any]], str | None]:\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"writesubtitles\": True,\n",
    "        \"subtitleslangs\": [\"vi\", \"en\"],\n",
    "        \"skip_download\": True,\n",
    "        \"extractor_args\": {\n",
    "            \"youtube\": {\"player_client\": [\"web\"]}\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            info_dict = ydl.extract_info(url, download=False)\n",
    "            subtitles = info_dict.get(\"subtitles\", {})\n",
    "            automatic_captions = info_dict.get(\"automatic_captions\", {})\n",
    "\n",
    "            subtitle_url = None\n",
    "            selected_lang = None\n",
    "\n",
    "            if \"vi\" in subtitles:\n",
    "                subtitle_url = subtitles[\"vi\"][0][\"url\"]\n",
    "                selected_lang = \"vi\"\n",
    "            elif \"en\" in subtitles:\n",
    "                subtitle_url = subtitles[\"en\"][0][\"url\"]\n",
    "                selected_lang = \"en\"\n",
    "            elif \"vi\" in automatic_captions:\n",
    "                subtitle_url = automatic_captions[\"vi\"][0][\"url\"]\n",
    "                selected_lang = \"vi (auto)\"\n",
    "            elif \"en\" in automatic_captions:\n",
    "                subtitle_url = automatic_captions[\"en\"][0][\"url\"]\n",
    "                selected_lang = \"en (auto)\"\n",
    "            elif subtitles:\n",
    "                first_lang = next(iter(subtitles))\n",
    "                subtitle_url = subtitles[first_lang][0][\"url\"]\n",
    "                selected_lang = f\"{first_lang} (manual)\"\n",
    "            elif automatic_captions:\n",
    "                first_lang = next(iter(automatic_captions))\n",
    "                subtitle_url = automatic_captions[first_lang][0][\"url\"]\n",
    "                selected_lang = f\"{first_lang} (auto)\"\n",
    "            else:\n",
    "                print(\"No subtitles found.\")\n",
    "                return None, None\n",
    "\n",
    "            transcript = ydl.urlopen(subtitle_url).read().decode(\"utf-8\")\n",
    "            transcript_dict = json.loads(transcript)\n",
    "            return extract_utf_from_events(transcript_dict), selected_lang\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while fetching subtitles: {e}\")\n",
    "            return None, None\n",
    "\n",
    "def extract_utf_from_events(data: dict) -> List[list[Any]]:\n",
    "    utf_scripts = []\n",
    "\n",
    "    for event in data[\"events\"]:\n",
    "        if \"segs\" in event:\n",
    "            utf_event = []\n",
    "            for seg in event[\"segs\"]:\n",
    "                if \"utf8\" in seg and seg[\"utf8\"] != \"\\n\":\n",
    "                    utf_event.append(seg[\"utf8\"])\n",
    "            if utf_event:\n",
    "                utf_scripts.append([event[\"tStartMs\"], \" \".join(utf_event)])\n",
    "\n",
    "    return utf_scripts\n",
    "\n",
    "def chunk_text(data: List[list[Any]], chunk_size: int = 250) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_word_count = 0\n",
    "    current_start_time = None\n",
    "\n",
    "    for start_time, text in data:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if current_chunk_word_count >= chunk_size:\n",
    "                chunks.append({\n",
    "                    \"text\": \" \".join(current_chunk),\n",
    "                    \"timestamp\": time_output(current_start_time)\n",
    "                })\n",
    "                current_chunk = []\n",
    "                current_chunk_word_count = 0\n",
    "                current_start_time = None\n",
    "\n",
    "            if current_start_time is None:\n",
    "                current_start_time = start_time\n",
    "\n",
    "            current_chunk.append(word)\n",
    "            current_chunk_word_count += 1\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            \"text\": \" \".join(current_chunk),\n",
    "            \"timestamp\": time_output(current_start_time)\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def time_output(time: int | None) -> str:\n",
    "    if time is None:\n",
    "        return \"00:00:00\"\n",
    "    return f\"{time // 3600000:02d}:{(time // 60000) % 60:02d}:{(time // 1000) % 60:02d}\"\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    safe_name = re.sub(r\"[^\\w\\s-]\", \"\", name).strip().replace(\" \", \"_\")\n",
    "    return safe_name\n",
    "\n",
    "def get_video_title(url: str) -> str:\n",
    "    \"\"\"Fetch the YouTube video title.\"\"\"\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"skip_download\": True,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        try:\n",
    "            info_dict = ydl.extract_info(url, download=False)\n",
    "            return info_dict.get(\"title\", \"unnamed_video\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching video title: {e}\")\n",
    "            return \"unnamed_video\"\n",
    "\n",
    "def create_json_file(url: str, scope: str, output_dir: str = \"./json_output\") -> str | None:\n",
    "    transcript_data, selected_lang = get_youtube_transcript(url)\n",
    "\n",
    "    if transcript_data:\n",
    "        transcript = \" \".join([x[1] for x in transcript_data])\n",
    "        chunks = chunk_text(transcript_data)\n",
    "\n",
    "        # Generate embeddings\n",
    "        chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        embeddings = embedder.encode(chunk_texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "        for chunk, embedding in zip(chunks, embeddings):\n",
    "            chunk[\"embedding\"] = embedding\n",
    "\n",
    "        # Build JSON structure matching PDF handler\n",
    "        json_data = {\n",
    "            \"type\": \"youtube\",\n",
    "            \"scope\": scope,\n",
    "            \"original_data\": url,\n",
    "            \"extracted_text\": transcript,\n",
    "            \"chunks\": chunks\n",
    "        }\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Use video title for filename\n",
    "        video_title = get_video_title(url)\n",
    "        safe_title = sanitize_filename(video_title)\n",
    "        output_path = os.path.join(output_dir, f\"{safe_title}.json\")\n",
    "\n",
    "        # Save JSON file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"JSON file created at: {output_path}\")\n",
    "        return output_path\n",
    "    else:\n",
    "        print(\"Failed to retrieve or process transcript.\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    url = \"https://www.youtube.com/watch?v=_nuQ39Y4T5Q\"\n",
    "    scope = \"test-scope\"\n",
    "\n",
    "    try:\n",
    "        json_file = create_json_file(url, scope)\n",
    "        if json_file:\n",
    "            print(f\"Generated JSON file: {json_file}\")\n",
    "        else:\n",
    "            print(\"No JSON file generated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroSE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
