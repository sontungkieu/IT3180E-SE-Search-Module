{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created at: ./json_output\\Computational_Intelligence_and_Neuroscience_-_2018_-_Voulodimos_-_Deep_Learning_for_Computer_Vision_A_Brief_Review.json\n",
      "Generated JSON file: ./json_output\\Computational_Intelligence_and_Neuroscience_-_2018_-_Voulodimos_-_Deep_Learning_for_Computer_Vision_A_Brief_Review.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained embedding model once\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Extract text from PDF with page numbers\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_with_pages = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\").strip()\n",
    "        if text:\n",
    "            text_with_pages.append((text, page_num + 1))  # (text, page number starting from 1)\n",
    "    \n",
    "    doc.close()\n",
    "    return text_with_pages\n",
    "\n",
    "def split_into_chunks(text_with_pages: List[Tuple[str, int]], chunk_size: int = 250) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Split text into chunks of approximately 250 words with page metadata\"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    word_count = 0\n",
    "    current_page = 1\n",
    "\n",
    "    all_words = []\n",
    "    for text, page in text_with_pages:\n",
    "        words = text.split()\n",
    "        all_words.extend([(word, page) for word in words])\n",
    "\n",
    "    for word, page in all_words:\n",
    "        current_chunk.append(word)\n",
    "        word_count += 1\n",
    "        current_page = page\n",
    "\n",
    "        if word_count >= chunk_size:\n",
    "            chunk_text = \" \".join(current_chunk)\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"timestamp\": f\"page_{current_page}\"\n",
    "            })\n",
    "            current_chunk = []\n",
    "            word_count = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunk_text = \" \".join(current_chunk)\n",
    "        chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"timestamp\": f\"page_{current_page}\"\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    name = os.path.splitext(os.path.basename(name))[0]\n",
    "    safe_name = re.sub(r\"[^\\w\\s-]\", \"\", name).strip().replace(\" \", \"_\")\n",
    "    return safe_name\n",
    "\n",
    "def create_json_file(pdf_path: str, output_dir: str = \"./json_output\") -> str:\n",
    "    \"\"\"Create a JSON file from PDF content with embeddings in YouTube-style format\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Extract text and chunk it\n",
    "    text_with_pages = extract_text_from_pdf(pdf_path)\n",
    "    extracted_text = \" \".join(text for text, _ in text_with_pages)\n",
    "    chunks = split_into_chunks(text_with_pages)\n",
    "\n",
    "    # Generate embeddings\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    embeddings = embedder.encode(chunk_texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        chunk[\"embedding\"] = embedding\n",
    "\n",
    "    # Build final JSON structure (no language_used key)\n",
    "    json_data = {\n",
    "        \"type\": \"pdf\",\n",
    "        \"scope\": sanitize_filename(pdf_path),\n",
    "        \"original_data\": pdf_path,\n",
    "        \"extracted_text\": extracted_text,\n",
    "        \"chunks\": chunks\n",
    "    }\n",
    "\n",
    "    # Save file using YouTube-style title\n",
    "    safe_title = sanitize_filename(pdf_path)\n",
    "    output_path = os.path.join(output_dir, f\"{safe_title}.json\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"JSON file created at: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"Computational Intelligence and Neuroscience - 2018 - Voulodimos - Deep Learning for Computer Vision A Brief Review.pdf\"\n",
    "    try:\n",
    "        json_file = create_json_file(pdf_path)\n",
    "        print(f\"Generated JSON file: {json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "software",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
